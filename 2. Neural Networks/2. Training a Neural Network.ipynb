{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is training? \n",
    "\n",
    "Training a neural network is a process that finds (or attempts to find) an optimal set of weights, such that the loss (the difference between the actual output and the predicted output) is minimized. We usually start with an arbitrary set of weights and iterate over each training example in order to gradually zero-in on the solution. Before we can train a network, we need to build it. This involves identifying the following parameters:\n",
    "- **Number of layers**\n",
    "- **Number of neurons** in each layer\n",
    "- **Activation Function**: It is applied to the output of a neuron. More on this below. \n",
    "- **Loss Function**: The loss function decides how to calculate the difference or the \"error\" between the predicted output and the real output of the training samples. \n",
    "- **Training Algorithm** (Optimizer): The optimizer decides how the weights will be updated.\n",
    "- **Batch Size**: The number of input samples that will lead to one update in the weights. If the batch size is too small, training will take a long time. If the batch size is very large, training won't converge properly.\n",
    "- **Learning Rate**: A hyperparameter that lets you control how much the weights will update in each step. If the learning rate is too small, the convergence will be delayed. If it is too large, convergence may never be achieved.\n",
    "\n",
    "# Training Process\n",
    "The training of a neural network is an iterative process. Each iteration consists of 2 passes - **forward pass** and **backward pass**.\n",
    "\n",
    "__Forward Pass__\n",
    "\n",
    "During the forward pass, the input sample is fed into the network, and its output is calculated. This output is compared to the actual output from the output label (we're doing supervised learning). The cost function calculates the error. Training the network can be thought of as an optimization problem, where we try to minimize the cost function. \n",
    "\n",
    "__Backward Pass__\n",
    "\n",
    "Now, during the backward pass, the errors are propagated backward. Using partial derivates, we try to find out how significant each feature is i.e. how much each feature contributes to the final output. Derivates help us find how much the final output changed with respect to a change in the input. Take a look at the mathematics behind this [here].(http://www.nunnlib.eu/home/mlp/back-propagation-algorithm)\n",
    "\n",
    "![img](https://cdn-images-1.medium.com/max/1600/1*q1M7LGiDTirwU-4LcFq7_Q.png)\n",
    "\n",
    "## Why do we need an activation function? \n",
    "The core operation that takes place at a neuron is a weighted sum, to which the bias is added. Depending on the value of the inputs, the output can be a positive or a negative number and can be infinitely large or infinitesimally small. Also, the output function may not be continuous or differentiable, which can prevent the backprop algorithm from working properly. In order to overcome these issues, we apply an activation function to the output of each neuron.\n",
    "An activation function can limit the output range of a neuron, and it also makes the output differentiable. \n",
    "Some popular activation functions are:\n",
    "- tanh\n",
    "- sigmoid\n",
    "- signum\n",
    "- ReLU (Rectified Linear Unit), Leaky ReLU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
